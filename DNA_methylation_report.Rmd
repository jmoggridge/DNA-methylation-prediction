---
title: "Prediction of DNA methylation state at CpG sites in the human genome: comparing the performance machine-learning methods"
author: "J Moggridge"
date: "05/04/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
log_reg_cv <- read_csv("./results/logreg_cv_results.csv") %>% 
  transmute(
    mean_fit_time,
    C = round(param_logistic__C,5),
    penalty = param_logistic__penalty,
    params,
    mean_test_score,
    std_test_score
  )
log_reg_cv_fig <- log_reg_cv %>% 
  filter(mean_test_score> 0.6) %>% 
  ggplot(aes(C, mean_test_score, color = penalty)) +
  geom_point() +
  geom_path() +
  scale_x_log10() +
  rcartocolor::scale_color_carto_d(palette = 1) +
  theme_minimal() +
  theme(legend.position = c(0.8, 0.8)) +
  labs(y = 'ROC AUC', x = 'regularization (C)', 
       subtitle = "Logistic regression hyperparameter tuning by 5-fold CV")

svm_cv <- read_csv("./results/svm_cv_results.csv") %>% 
  transmute(
    mean_fit_time,
    C = round(param_svm__C,3),
    gamma = factor(round(param_svm__gamma, 3)),
    params,
    mean = mean_test_score,
    std_test_score
  )
svm_cv %>% 
  filter(mean >0.6) %>% 
  
  group_by(gamma) %>% 
  ggplot(aes(x = C, y = mean, color = gamma)) +
  geom_point() +
  geom_path() +
  scale_x_log10() +
  rcartocolor::scale_color_carto_d(palette = 1) +
  theme_minimal() 

rf_cv <- read_csv("./results/rf_cv_results.csv") %>% 
  transmute(
    estimators = param_rf__n_estimators,
    min_sample_leaf = factor(param_rf__min_samples_leaf),
    max_features = param_rf__max_features,
    mean = mean_test_score,
    std_err = std_test_score
  )
rf_cv %>% 
  ggplot(aes(x = estimators, y= mean, color = min_sample_leaf)) +
  geom_path() +
  geom_point() +
  scale_x_log10() +
  facet_wrap(~max_features) +
  rcartocolor::scale_color_carto_d(palette = 1) +
  theme_minimal() 
  
```


## Abstract

DNA methylation is an epigenetic modification of the DNA that can impact gene expression. Many biological processes are mediated through methylation of specific sites and aberrant methylation is linked with disease states.

Prediction of DNA methylation status is of interest ...

Sequence features were extracted in the form of one-hot-encodings of 120 bp around the site (+/- 60bp) and k-mer frequencies (dinucleotides) in the 1000 bp around the site.

I evaluated the performance of several machine learning methods on the DNA methylation problem.

I found that... model is the best ...

In predicting unseen data....


## Introduction

DNA methylation is an epigenetic modification that impacts many processes and is involved in disease states..

... Imprinting

.... Measurement is noisy?

Prediction of methylation status could be important for...

The DNA methylation dataset used in this work is comprised of positional, categorical, and sequence information. As such, the data requires feature extraction to make use of the non-numeric information, particularly the sequence information. Nominal information include (1) the relation to any nearby CpG island (island, north/south shelf or shore, or none); (2) location relative to genes (TSS regions, UTRs, gene body, 1st exon); (3) associated regulatory features (gene/non-gene/promoter associations & cell-type specificity).

The positional information include the location within the genome and the position of any nearby CpG islands. The training data is comprised entirely of sites on chromosomes 1-10 and the test data has sites from chromosomes 11-22. I extracted the distance to the nearest island (if one exist) and retained only this information, as the exact location information would be easy for ML models to memorize.

Two popular approaches to DNA sequence processing are to use a 'bag-of-words' approach with k-mers, or to use one-hot encoding for each position. One-hot encoding the sequence has the advantage of retaining positional information whereas the k-mer counts only retain compositional (or motif) information. There is 2 kbp of sequence centered around the CpG sites included in the dataset. I used a combined approach, where I one-hot encoded the 60 bp upstream and downstream regions and counted kmers over the 2 kbp region.

<!-- Use of dimensionality reduction might be appropriate when the number of such features gets very large. -->

I sought to find out which model performs best on the problem of predicting DNA methylation status at CpG sites.

<!-- Beta 0= unmethylated, Beta 1=methylated) -->

## Methods

### Data preparation

The DNA methylation dataset used in this work is comprised of positional, categorical, and sequence information. As such, the data requires feature extraction to make use of the non-numeric information, particularly the sequence information. Nominal information include (1) the relation to any nearby CpG island (island, north/south shelf or shore, or none); (2) location relative to genes (TSS regions, UTRs, gene body, 1st exon); (3) associated regulatory features (gene/non-gene/promoter associations & cell-type specificity).

The positional information include the location within the genome and the position of any nearby CpG islands. The training data is comprised entirely of sites on chromosomes 1-10 and the test data has sites from chromosomes 11-22. I extracted the distance to the nearest island (if one exist) and retained only this information, as the exact location information would be easy for ML models to memorize.

Two popular approaches to DNA sequence processing are to use a 'bag-of-words' approach with k-mers, or to use one-hot encoding for each position. One-hot encoding the sequence has the advantage of retaining positional information whereas the kmer counts only retain compositional (or motif) information. There is 2 kbp of sequence centered around the CpG sites included in the dataset. I used a combinged approach, where I one-hot encoded the 60 bp upstream and downstream regions and counted kmers over the 2 kbp region.

Use of dimensionality reduction might be appropriate when the number of such features gets very large.

### Model selection

All model selections and evaluations were performed with the sci-kit learn library.

I first evaluated a basic logistic regression model (l2 norm, C=1.0) to establish a baseline for performance. This model has AUC of 0.923 in cross-validation, which is good, but a surprisingly large AUC of 0.976 on the validation set. If other more complicated models do not perform much better, we would consider this model as it is faster to train and simpler to interpret than SVM, random forest, k-NN, etc., and we can find out which predictors are most important from the model coefficients.

I then tried to improve on the basic logistic regression model by tuning the logistic regression hyperparameters: models had either Lasso or Ridge regularization and penalty (C) values from a geometric distribution over 0.0001 to 1000. 
The best model in a random search (50 iterations) was a Lasso (l1 norm) model with C = 0.0336. had an AUC of 0.946 in 5-fold CV and 0.985 on the validation set. 

```{r}
log_reg_cv_fig
```

```
Best score (ROC AUC=0.946)
'logistic__solver': 'liblinear', 
'logistic__penalty': 'l1', 
'logistic__C': 0.03359818286283781

              precision    recall  f1-score   support

           0       0.97      0.96      0.96     16194
           1       0.91      0.93      0.92      7058

    accuracy                           0.95     23252
   macro avg       0.94      0.94      0.94     23252
weighted avg       0.95      0.95      0.95     23252


[[15534   660]
 [  502  6556]]

Test performance of Logistic regression model   
          algortithm  roc_auc  accuracy  precision    recall        F1
 Logistic regression  0.98546  0.950026   0.908537  0.928875  0.918593

```


scp jmoggrid@cedar.computecanada:/scratch/jmoggrid/DNA_methylation/results/* ./results/


To see if a non-linear solution was required to improve the classification performance above 95%, I applied Radial Basis Function (rbf) kernel SVMs in 5-fold cross-validation. The hyperparameters were tuned in a random search over gamma values from ... and C values....


```
91 min
RandomizedSearchCV(
  cv=5,
  estimator=Pipeline(steps=[('scaler', StandardScaler()),
  ('svm', SVC(random_state=0))]),
  n_iter=40, n_jobs=-1,
  param_distributions={
    'svm__C': array([
      1e-02, 4.64e-02, 2.1e-01, 1e+00, 4.6e+00, 2.1e+01, 1+02, 4.6e+02,
      2.15e+03, 1e+04
      ]),
  'svm__gamma': array([
      1e-03, 4.64e-03, 2.15e-02, 1e-01, 4.64e-01, 2.1e+00, 1e+01, 4e01,
      202, 1e+03
      ])},
  scoring=make_scorer(roc_auc_score), verbose=3)

* it used the smallest gamma value and medium in C
Best SVM score (CV score=0.933):
Best SVM parameters: {'svm__gamma': 0.001, 'svm__C': 1.0}

              precision    recall  f1-score   support

           0       0.96      0.96      0.96     16194
           1       0.90      0.91      0.90      7058

    accuracy                           0.94     23252
   macro avg       0.93      0.93      0.93     23252
weighted avg       0.94      0.94      0.94     23252

[[15496   698]
 [  653  6405]]

Test performance of SVM model
  algortithm   roc_auc  accuracy  precision    recall        F1
0        SVM  0.932189  0.941897   0.901732  0.907481  0.904597

```

The best model in cross-validation had 
The ROC AUC was..
On the validation set, the ROC AUC ....

```
Best RF score (CV score=0.915):
Best RF params
{'rf__n_estimators': 200, 'rf__min_samples_leaf': 10, 
'rf__max_features': 'auto', 'rf__bootstrap': True}


              precision    recall  f1-score   support

           0       0.94      0.96      0.95     16194
           1       0.91      0.87      0.89      7058

    accuracy                           0.93     23252
   macro avg       0.93      0.92      0.92     23252
weighted avg       0.93      0.93      0.93     23252

[[15589   605]
 [  932  6126]]

      algortithm   roc_auc  accuracy  precision    recall        F1
0  Random forest  0.915296  0.933898   0.910117  0.867951  0.888534

Best RF2 score (CV score=0.924):
{'rf__n_estimators': 50, 'rf__min_samples_split': 20, 'rf__min_samples_leaf': 1, 'rf__max_features': 'auto', 'rf__max_depth': 100}
              precision    recall  f1-score   support

           0       0.95      0.97      0.96     16194
           1       0.92      0.88      0.90      7058

    accuracy                           0.94     23252
   macro avg       0.93      0.92      0.93     23252
weighted avg       0.94      0.94      0.94     23252

[[15630   564]
 [  882  6176]]

        algortithm   roc_auc  accuracy  precision    recall        F1
0  Random forest 2  0.920104  0.937812    0.91632  0.875035  0.895202```

### Model evaluation
